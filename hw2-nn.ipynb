{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическое задание\n",
    "\n",
    "<ol>\n",
    "    <li>Попробуйте обучить нейронную сеть на Keras с другими параметрами. \n",
    "        Опишите в комментарии к уроку - какой результата вы добились от нейросети? Что помогло вам улучшить ее точность?</li>\n",
    "    <li>Поработайте с документацией Keras.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The full neural network code!\n",
    "###############################\n",
    "import numpy as np\n",
    "import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 42\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "\n",
    "# Normalize the images.\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "\n",
    "# Flatten the images.\n",
    "train_images = train_images.reshape((-1, 784))\n",
    "test_images = test_images.reshape((-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Dense(64, activation='elu', input_shape=(784,)),\n",
    "  Dense(64, activation='elu'),\n",
    "    Dense(64, activation='elu'),\n",
    "#         Dense(64, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Dense(64, activation='relu', input_shape=(784,)),\n",
    "  Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  optimizer='Adadelta',\n",
    "  loss='cosine_similarity',\n",
    "  metrics=['top_k_categorical_accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  optimizer='Adam',\n",
    "  loss='cosine_similarity',\n",
    "  metrics=['top_k_categorical_accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: -0.9742 - top_k_categorical_accuracy: 0.9988\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: -0.9750 - top_k_categorical_accuracy: 0.9988\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: -0.9746 - top_k_categorical_accuracy: 0.9986\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: -0.9757 - top_k_categorical_accuracy: 0.9990\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: -0.9757 - top_k_categorical_accuracy: 0.9989\n",
      "10000/10000 [==============================] - 0s 20us/step\n",
      "[7 2 1 0 4 1 4 9 5 9]\n",
      "[7 2 1 0 4 1 4 9 5 9]\n",
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=5,\n",
    "  batch_size=32,\n",
    ")\n",
    "\n",
    "# Evaluate the model.\n",
    "model.evaluate(\n",
    "  test_images,\n",
    "  to_categorical(test_labels)\n",
    ")\n",
    "\n",
    "# Save the model to disk.\n",
    "# model.save_weights('model1.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('model.h5')\n",
    "\n",
    "\n",
    "# Predict on the first 10 test images.\n",
    "predictions = model.predict(test_images[:100])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions[:10], axis=1)) # [7, 2, 1, 0, 4]\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:10]) # [7, 2, 1, 0, 4]\n",
    "\n",
    "print(f'Accuracy: {accuracy(test_labels[:100], np.argmax(predictions, axis=1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: -0.8981 - top_k_categorical_accuracy: 0.9872\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: -0.9432 - top_k_categorical_accuracy: 0.9959\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: -0.9539 - top_k_categorical_accuracy: 0.9970\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: -0.9590 - top_k_categorical_accuracy: 0.9978\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: -0.9626 - top_k_categorical_accuracy: 0.9981\n",
      "10000/10000 [==============================] - 0s 23us/step\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=5,\n",
    "  batch_size=32,\n",
    ")\n",
    "\n",
    "# Evaluate the model.\n",
    "model.evaluate(\n",
    "  test_images,\n",
    "  to_categorical(test_labels)\n",
    ")\n",
    "\n",
    "# Save the model to disk.\n",
    "model.save_weights('model2.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('model.h5')\n",
    "\n",
    "\n",
    "# Predict on the first 10 test images.\n",
    "predictions = model.predict(test_images[:10])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:10]) # [7, 2, 1, 0, 4]\n",
    "\n",
    "print(f'Accuracy: {accuracy(test_labels, np.argmax(predictions, axis=1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: -0.8977 - top_k_categorical_accuracy: 0.9869\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: -0.9494 - top_k_categorical_accuracy: 0.9966\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: -0.9603 - top_k_categorical_accuracy: 0.9976\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: -0.9657 - top_k_categorical_accuracy: 0.9983\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: -0.9695 - top_k_categorical_accuracy: 0.9982\n",
      "10000/10000 [==============================] - 0s 21us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=5,\n",
    "  batch_size=32,\n",
    ")\n",
    "\n",
    "# Evaluate the model.\n",
    "model.evaluate(\n",
    "  test_images,\n",
    "  to_categorical(test_labels)\n",
    ")\n",
    "\n",
    "# Save the model to disk.\n",
    "model.save_weights('model3.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('model.h5')\n",
    "\n",
    "\n",
    "# Predict on the first 10 test images.\n",
    "predictions = model.predict(test_images[:10])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:10]) # [7, 2, 1, 0, 4]\n",
    "\n",
    "print(f'Accuracy: {accuracy(test_labels, np.argmax(predictions, axis=1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперименты и выводы\n",
    "Если убрать из нормализации центровку на 0, то качество растёт на 1-2%\n",
    "Попробовал заменить при этом relu на softplus - метрики упали; на tanh - метрики вернулись.\n",
    "\n",
    "При сокращении до двух слоёв качество падает. При этом, если поставить на вход softmax, начинаются ошибки. Точость около 0.7. Увеличение слоёв не даёт значительного прироста качества.\n",
    "\n",
    "Заменил оптимайзер на Adadelta, после чего сразу получил более 98% по всем эпохам. Вероятно, из-за того, что не перегрузил данные.\n",
    "Nadam даёт 97% ; SGD - низкий результат 92% и ошибки.\n",
    "\n",
    "Метрика top_k_categorical_accuracy дала прирост до 99% по всем эпохам. categorical_accuracy - вернула к значениям вокруг 95%\n",
    "При оптимайзере Adam показатели также высокие (99%), но есть ошибки, что говорит о переобучении.\n",
    "\n",
    "### Вывод:\n",
    "Усложнение сети не даёт значимого прироста. Но и не влияет на производительность на простом примере.\n",
    "Максимальный эффект дают настройки модели  \n",
    "   * optimizer='Adadelta',\n",
    "   * loss='cosine_similarity',\n",
    "   * metrics='top_k_categorical_accuracy'\n",
    "\n",
    "\n",
    "Использовались параметры из документации: \n",
    "https://keras.io/api/losses/\n",
    "https://keras.io/api/optimizers/\n",
    "https://keras.io/api/metrics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
